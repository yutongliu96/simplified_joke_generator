{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Fr-mdwxib4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install http://download.pytorch.org/whl/cu80/torch-0.4.1-cp36-cp36m-linux_x86_64.whl \n",
        "# !pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30V1nrmJxjIi",
        "colab_type": "code",
        "outputId": "544cfe8a-8701-4aea-e80b-8bcbb9041892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import time\n",
        "\n",
        "print(torch.__version__)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c__O8PTzR-3-",
        "colab_type": "code",
        "outputId": "4fb98c32-7a83-4da7-dee6-2932283ab2bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rddl6G_2Sozl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "topics = []\n",
        "jokes = []\n",
        "with open('/gdrive/My Drive/train_data2.txt', encoding='ISO-8859-1') as f1:\n",
        "  for line in f1:\n",
        "    jokes.append(line[:-1])\n",
        "with open('/gdrive/My Drive/pos_tags2.txt', encoding='ISO-8859-1') as f2:\n",
        "  for line in f2:\n",
        "    topics.append(line[:-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMwOiSaiSzOj",
        "colab_type": "code",
        "outputId": "78f8d418-bd48-4b6e-d773-d0a2be0b329f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "\n",
        "num_examples = 15783 \n",
        "\n",
        "# creates lists containing each pair\n",
        "original_word_pairs = [[topics[i], jokes[i]] for i in range(num_examples)]\n",
        "data = pd.DataFrame(original_word_pairs, columns=[\"topics\", \"jokes\"])\n",
        "data.head(5)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topics</th>\n",
              "      <th>jokes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hey guy twitter</td>\n",
              "      <td>yes , i know what you guys are thinking , \" he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i m glad cable truth i ve talk show host cable i</td>\n",
              "      <td>i ' m glad to be on cable . the truth is , i '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i m tbs demographicpeople afford hbo</td>\n",
              "      <td>i ' m happy to report that we ' re already # 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>night show channel lot money viewers trouble f...</td>\n",
              "      <td>it ' s not easy doing a late - night show on a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>man show business blackest channel tv</td>\n",
              "      <td>that ' s rightthe whitest man in show busines...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              topics                                              jokes\n",
              "0                                    hey guy twitter  yes , i know what you guys are thinking , \" he...\n",
              "1   i m glad cable truth i ve talk show host cable i  i ' m glad to be on cable . the truth is , i '...\n",
              "2              i m tbs demographicpeople afford hbo  i ' m happy to report that we ' re already # 1...\n",
              "3  night show channel lot money viewers trouble f...  it ' s not easy doing a late - night show on a...\n",
              "4              man show business blackest channel tv  that ' s rightthe whitest man in show busines..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob3R-opsTzvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    \"\"\"\n",
        "    Normalizes latin chars with accent to their canonical decomposition\n",
        "    \"\"\"\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaw5yNiST3gU",
        "colab_type": "text"
      },
      "source": [
        "##Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEWCSqk7T96y",
        "colab_type": "code",
        "outputId": "cd1a211c-f524-4dfb-d993-898ec5e9d0cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Now we do the preprocessing using pandas and lambdas\n",
        "data[\"topics\"] = data.topics.apply(lambda w: preprocess_sentence(w))\n",
        "data[\"jokes\"] = data.jokes.apply(lambda w: preprocess_sentence(w))\n",
        "data.sample(10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topics</th>\n",
              "      <th>jokes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10472</th>\n",
              "      <td>&lt;start&gt; sister birth state art delivery room t...</td>\n",
              "      <td>&lt;start&gt; my sister gave birth in a state of the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8713</th>\n",
              "      <td>&lt;start&gt; donald trump time magazine person year...</td>\n",
              "      <td>&lt;start&gt; donald trump has been named time magaz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5284</th>\n",
              "      <td>&lt;start&gt; mother day moms hooters s way mother s...</td>\n",
              "      <td>&lt;start&gt; on mother s day , moms can eat for fre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12252</th>\n",
              "      <td>&lt;start&gt; yo mama people exercize &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; yo mama is so fat people run around he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9360</th>\n",
              "      <td>&lt;start&gt; invention people kernels corn face fir...</td>\n",
              "      <td>&lt;start&gt; before the invention of popcorn , peop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10968</th>\n",
              "      <td>&lt;start&gt; lord strength i clutches cholesterol p...</td>\n",
              "      <td>&lt;start&gt; lord , grant me the strength that i ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5836</th>\n",
              "      <td>&lt;start&gt; weekend george clooney italy bachelor ...</td>\n",
              "      <td>&lt;start&gt; over the weekend , george clooney got ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3711</th>\n",
              "      <td>&lt;start&gt; tremor yesterday la earthquake asia &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; felt a big tremor yesterday in la . th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14151</th>\n",
              "      <td>&lt;start&gt; penis condom m &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; what did the penis say to the condom ?...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15183</th>\n",
              "      <td>&lt;start&gt; definition people country &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; definition of alien people from anothe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  topics                                              jokes\n",
              "10472  <start> sister birth state art delivery room t...  <start> my sister gave birth in a state of the...\n",
              "8713   <start> donald trump time magazine person year...  <start> donald trump has been named time magaz...\n",
              "5284   <start> mother day moms hooters s way mother s...  <start> on mother s day , moms can eat for fre...\n",
              "12252              <start> yo mama people exercize <end>  <start> yo mama is so fat people run around he...\n",
              "9360   <start> invention people kernels corn face fir...  <start> before the invention of popcorn , peop...\n",
              "10968  <start> lord strength i clutches cholesterol p...  <start> lord , grant me the strength that i ma...\n",
              "5836   <start> weekend george clooney italy bachelor ...  <start> over the weekend , george clooney got ...\n",
              "3711   <start> tremor yesterday la earthquake asia <end>  <start> felt a big tremor yesterday in la . th...\n",
              "14151                       <start> penis condom m <end>  <start> what did the penis say to the condom ?...\n",
              "15183            <start> definition people country <end>  <start> definition of alien people from anothe..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7adSFag-WZH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n",
        "# (e.g., 5 -> \"dad\") for each language,\n",
        "class LanguageIndex():\n",
        "    def __init__(self, lang):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab = set()\n",
        "        \n",
        "        self.create_index()\n",
        "        \n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            self.vocab.update(phrase.split(' '))\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx['<pad>'] = 0\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.vocab):\n",
        "            self.word2idx[word] = index + 1 # +1 because of pad token\n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4OQtshbWfV5",
        "colab_type": "code",
        "outputId": "f8959553-adb5-4dc0-cea3-0a30f90c4b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# index language using the class above\n",
        "inp_lang = LanguageIndex(data[\"topics\"].values.tolist())\n",
        "targ_lang = LanguageIndex(data[\"jokes\"].values.tolist())\n",
        "# Vectorize the input and target languages\n",
        "input_tensor = [[inp_lang.word2idx[s] for s in topic.split(' ')]  for topic in data[\"topics\"].values.tolist()]\n",
        "target_tensor = [[targ_lang.word2idx[s] for s in joke.split(' ')]  for joke in data[\"jokes\"].values.tolist()]\n",
        "input_tensor[15:25]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 12366, 2396, 388, 6449, 10543, 7061, 10508, 2],\n",
              " [3, 1968, 3108, 11362, 9794, 3307, 9743, 4832, 12897, 11362, 12473, 1077, 2],\n",
              " [3,\n",
              "  8614,\n",
              "  10879,\n",
              "  11801,\n",
              "  13012,\n",
              "  740,\n",
              "  13965,\n",
              "  1287,\n",
              "  739,\n",
              "  1056,\n",
              "  4173,\n",
              "  13012,\n",
              "  10879,\n",
              "  9321,\n",
              "  2],\n",
              " [3, 1586, 9895, 13895, 5220, 12083, 12057, 9269, 1101, 2],\n",
              " [3, 12366, 7990, 6464, 9727, 12366, 7747, 4287, 2],\n",
              " [3, 4639, 6895, 7681, 3193, 7681, 997, 12156, 6203, 5809, 6895, 2],\n",
              " [3, 11647, 472, 12063, 13363, 2683, 2299, 2299, 9419, 13729, 9419, 2],\n",
              " [3, 14112, 5081, 12147, 2295, 12881, 14004, 6917, 2],\n",
              " [3, 13691, 9764, 11399, 3928, 9389, 13765, 8244, 3495, 2990, 10879, 6603, 2],\n",
              " [3, 4221, 10564, 10879, 3766, 10564, 13625, 2879, 615, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRPR1akKWnOU",
        "colab_type": "code",
        "outputId": "a5e80df8-8f76-4df4-9246-5ba8dc2fba32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "target_tensor[:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[6,\n",
              "  20324,\n",
              "  3,\n",
              "  8828,\n",
              "  10018,\n",
              "  19901,\n",
              "  20352,\n",
              "  7939,\n",
              "  845,\n",
              "  18257,\n",
              "  3,\n",
              "  8365,\n",
              "  3,\n",
              "  9484,\n",
              "  15572,\n",
              "  18197,\n",
              "  7937,\n",
              "  7184,\n",
              "  18917,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  8828,\n",
              "  10792,\n",
              "  7524,\n",
              "  18427,\n",
              "  1491,\n",
              "  12554,\n",
              "  2548,\n",
              "  4,\n",
              "  18197,\n",
              "  18811,\n",
              "  9454,\n",
              "  3,\n",
              "  8828,\n",
              "  19385,\n",
              "  5450,\n",
              "  12475,\n",
              "  1596,\n",
              "  8,\n",
              "  17919,\n",
              "  16319,\n",
              "  8638,\n",
              "  12554,\n",
              "  1435,\n",
              "  2548,\n",
              "  6194,\n",
              "  16440,\n",
              "  8828,\n",
              "  19742,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  8828,\n",
              "  10792,\n",
              "  8101,\n",
              "  18427,\n",
              "  15023,\n",
              "  18193,\n",
              "  19799,\n",
              "  14636,\n",
              "  509,\n",
              "  9007,\n",
              "  18009,\n",
              "  15572,\n",
              "  9891,\n",
              "  4768,\n",
              "  13225,\n",
              "  19967,\n",
              "  2641,\n",
              "  17870,\n",
              "  314,\n",
              "  8205,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  9484,\n",
              "  15572,\n",
              "  12291,\n",
              "  5678,\n",
              "  5301,\n",
              "  8,\n",
              "  10182,\n",
              "  12181,\n",
              "  16319,\n",
              "  12554,\n",
              "  8,\n",
              "  2977,\n",
              "  20110,\n",
              "  8,\n",
              "  10691,\n",
              "  12475,\n",
              "  11675,\n",
              "  616,\n",
              "  18193,\n",
              "  19503,\n",
              "  8183,\n",
              "  18778,\n",
              "  6736,\n",
              "  4,\n",
              "  16764,\n",
              "  18193,\n",
              "  15572,\n",
              "  19988,\n",
              "  8828,\n",
              "  10302,\n",
              "  12030,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  18193,\n",
              "  15572,\n",
              "  15281,\n",
              "  18197,\n",
              "  19962,\n",
              "  10914,\n",
              "  9007,\n",
              "  16319,\n",
              "  2493,\n",
              "  9454,\n",
              "  1226,\n",
              "  12554,\n",
              "  18197,\n",
              "  15938,\n",
              "  1829,\n",
              "  2977,\n",
              "  12554,\n",
              "  18886,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  5649,\n",
              "  18437,\n",
              "  3,\n",
              "  7038,\n",
              "  13975,\n",
              "  7440,\n",
              "  19639,\n",
              "  4,\n",
              "  2488,\n",
              "  763,\n",
              "  12554,\n",
              "  12617,\n",
              "  20056,\n",
              "  4,\n",
              "  19921,\n",
              "  959,\n",
              "  60,\n",
              "  1596,\n",
              "  18197,\n",
              "  10250,\n",
              "  12475,\n",
              "  18197,\n",
              "  7124,\n",
              "  20187,\n",
              "  3,\n",
              "  12617,\n",
              "  20056,\n",
              "  15606,\n",
              "  3,\n",
              "  9484,\n",
              "  15572,\n",
              "  12291,\n",
              "  1254,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  775,\n",
              "  9792,\n",
              "  10204,\n",
              "  9494,\n",
              "  12567,\n",
              "  17373,\n",
              "  9007,\n",
              "  3162,\n",
              "  4,\n",
              "  775,\n",
              "  15747,\n",
              "  18267,\n",
              "  9454,\n",
              "  593,\n",
              "  6261,\n",
              "  12607,\n",
              "  18427,\n",
              "  15995,\n",
              "  9423,\n",
              "  18427,\n",
              "  18197,\n",
              "  19448,\n",
              "  9929,\n",
              "  19967,\n",
              "  10875,\n",
              "  18208,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  8,\n",
              "  13667,\n",
              "  12493,\n",
              "  9007,\n",
              "  10636,\n",
              "  9454,\n",
              "  9007,\n",
              "  18778,\n",
              "  6987,\n",
              "  475,\n",
              "  16604,\n",
              "  14818,\n",
              "  18427,\n",
              "  16853,\n",
              "  9347,\n",
              "  8443,\n",
              "  12496,\n",
              "  15029,\n",
              "  4,\n",
              "  1119,\n",
              "  1531,\n",
              "  17746,\n",
              "  19921,\n",
              "  18238,\n",
              "  14645,\n",
              "  18197,\n",
              "  13022,\n",
              "  18193,\n",
              "  15606,\n",
              "  18375,\n",
              "  12475,\n",
              "  4579,\n",
              "  8828,\n",
              "  10470,\n",
              "  1725,\n",
              "  2518,\n",
              "  616,\n",
              "  8828,\n",
              "  2677,\n",
              "  10428,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  5649,\n",
              "  18267,\n",
              "  19838,\n",
              "  3,\n",
              "  646,\n",
              "  4775,\n",
              "  14209,\n",
              "  13975,\n",
              "  12395,\n",
              "  15572,\n",
              "  19561,\n",
              "  18427,\n",
              "  9097,\n",
              "  4,\n",
              "  756,\n",
              "  3,\n",
              "  12721,\n",
              "  12475,\n",
              "  9098,\n",
              "  1609,\n",
              "  8210,\n",
              "  15572,\n",
              "  593,\n",
              "  562,\n",
              "  4,\n",
              "  5],\n",
              " [6,\n",
              "  542,\n",
              "  4,\n",
              "  3556,\n",
              "  9454,\n",
              "  3586,\n",
              "  19034,\n",
              "  6758,\n",
              "  6987,\n",
              "  15998,\n",
              "  8,\n",
              "  2047,\n",
              "  60,\n",
              "  13167,\n",
              "  4,\n",
              "  8872,\n",
              "  20352,\n",
              "  18256,\n",
              "  18193,\n",
              "  15572,\n",
              "  1254,\n",
              "  3,\n",
              "  20352,\n",
              "  16305,\n",
              "  15960,\n",
              "  19901,\n",
              "  542,\n",
              "  15747,\n",
              "  2523,\n",
              "  12475,\n",
              "  18197,\n",
              "  2047,\n",
              "  11434,\n",
              "  511,\n",
              "  10470,\n",
              "  4,\n",
              "  5]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzLnKXSIXRri",
        "colab_type": "code",
        "outputId": "4f1109bb-ffa4-49e5-a4ca-98b8a91bc098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "# calculate the max_length of input and output tensor\n",
        "max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
        "(max_length_inp, max_length_tar)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45, 98)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-kgIOv4XeHt",
        "colab_type": "code",
        "outputId": "bcfa0ad6-2ae3-4b3b-f3bf-7f7ea5413683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def pad_sequences(x, max_len):\n",
        "    padded = np.zeros((max_len), dtype=np.int64)\n",
        "    if len(x) > max_len: padded[:] = x[:max_len]\n",
        "    else: padded[:len(x)] = x\n",
        "    return padded\n",
        "input_tensor = [pad_sequences(x, max_length_inp) for x in input_tensor]\n",
        "target_tensor = [pad_sequences(x, max_length_tar) for x in target_tensor]\n",
        "len(input_tensor)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rmfY6aCX9mr",
        "colab_type": "code",
        "outputId": "bb98d337-7f83-464d-dcdc-b4ecd81c72d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12626, 12626, 3157, 3157)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWJxnGVmYBG8",
        "colab_type": "text"
      },
      "source": [
        "##Load data into DataLoader for Batching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo3Q9IukYEIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEuVbtZMYRu2",
        "colab_type": "text"
      },
      "source": [
        "##Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TuhfwfDYUHw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word2idx)\n",
        "vocab_tar_size = len(targ_lang.word2idx)\n",
        "\n",
        "train_dataset = MyData(input_tensor_train, target_tensor_train)\n",
        "val_dataset = MyData(input_tensor_val, target_tensor_val)\n",
        "\n",
        "dataset = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hivDwtYYvYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim, self.enc_units)\n",
        "        \n",
        "    def forward(self, x, lens, device):\n",
        "        # x: batch_size, max_length \n",
        "        \n",
        "        # x: batch_size, max_length, embedding_dim\n",
        "        x = self.embedding(x) \n",
        "                \n",
        "        # x transformed = max_len X batch_size X embedding_dim\n",
        "        # x = x.permute(1,0,2)\n",
        "        x = pack_padded_sequence(x, lens) # unpad\n",
        "    \n",
        "        self.hidden = self.initialize_hidden_state(device)\n",
        "        \n",
        "        # output: max_length, batch_size, enc_units\n",
        "        # self.hidden: 1, batch_size, enc_units\n",
        "        output, self.hidden = self.gru(x, self.hidden) # gru returns hidden state of all timesteps as well as hidden state at last timestep\n",
        "        \n",
        "        # pad the sequence to the max length in the batch\n",
        "        output, _ = pad_packed_sequence(output)\n",
        "        \n",
        "        return output, self.hidden\n",
        "\n",
        "    def initialize_hidden_state(self, device):\n",
        "        return torch.zeros((1, self.batch_sz, self.enc_units)).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sHXXNN_Zz4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### sort batch function to be able to use with pad_packed_sequence\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X.transpose(0,1), y, lengths # transpose (batch x seq) to (seq x batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khbcam9-Z2jj",
        "colab_type": "text"
      },
      "source": [
        "##Testing the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6jbpvMgZ9yw",
        "colab_type": "code",
        "outputId": "f7a2f3f0-ea2b-48b7-e7f4-2c1a3eed8485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "### Testing Encoder part\n",
        "# TODO: put whether GPU is available or not\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "\n",
        "print(enc_output.size()) # max_length, batch_size, enc_units"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([26, 64, 1024])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cicnH9Fya2cM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, enc_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.enc_units = enc_units\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.gru = nn.GRU(self.embedding_dim + self.enc_units, \n",
        "                          self.dec_units,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(self.enc_units, self.vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.W1 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.W2 = nn.Linear(self.enc_units, self.dec_units)\n",
        "        self.V = nn.Linear(self.enc_units, 1)\n",
        "    \n",
        "    def forward(self, x, hidden, enc_output):\n",
        "        # enc_output original: (max_length, batch_size, enc_units)\n",
        "        # enc_output converted == (batch_size, max_length, hidden_size)\n",
        "        enc_output = enc_output.permute(1,0,2)\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        \n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        hidden_with_time_axis = hidden.permute(1, 0, 2)\n",
        "        \n",
        "        # score: (batch_size, max_length, hidden_size)\n",
        "        score = torch.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis))\n",
        "        \n",
        "        #score = torch.tanh(self.W2(hidden_with_time_axis) + self.W1(enc_output))\n",
        "          \n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        attention_weights = torch.softmax(self.V(score), dim=1)\n",
        "        \n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * enc_output\n",
        "        context_vector = torch.sum(context_vector, dim=1)\n",
        "        \n",
        "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "        #x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        x = torch.cat((context_vector.unsqueeze(1), x), -1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        # output: (batch_size, 1, hidden_size)\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output =  output.view(-1, output.size(2))\n",
        "        \n",
        "        # output shape == (batch_size * 1, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return torch.zeros((1, self.batch_sz, self.dec_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93Uya9Gya3KT",
        "colab_type": "code",
        "outputId": "c336c112-4606-4525-ddc6-ef73a9f211cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "# obtain one sample from the data iterator\n",
        "it = iter(dataset)\n",
        "x, y, x_len = next(it)\n",
        "\n",
        "print(\"Input: \", x.shape)\n",
        "print(\"Output: \", y.shape)\n",
        "\n",
        "# sort the batch first to be able to use with pac_pack_sequence\n",
        "xs, ys, lens = sort_batch(x, y, x_len)\n",
        "\n",
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "print(\"Encoder Output: \", enc_output.shape) # batch_size X max_length X enc_units\n",
        "print(\"Encoder Hidden: \", enc_hidden.shape) # batch_size X enc_units (corresponds to the last state)\n",
        "\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "#print(enc_hidden.squeeze(0).shape)\n",
        "\n",
        "dec_hidden = enc_hidden#.squeeze(0)\n",
        "dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "print(\"Decoder Input: \", dec_input.shape)\n",
        "print(\"--------\")\n",
        "\n",
        "for t in range(1, y.size(1)):\n",
        "    # enc_hidden: 1, batch_size, enc_units\n",
        "    # output: max_length, batch_size, enc_units\n",
        "    predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "    \n",
        "    print(\"Prediction: \", predictions.shape)\n",
        "    print(\"Decoder Hidden: \", dec_hidden.shape)\n",
        "    \n",
        "    #loss += loss_function(y[:, t].to(device), predictions.to(device))\n",
        "    \n",
        "    dec_input = y[:, t].unsqueeze(1)\n",
        "    print(dec_input.shape)\n",
        "    break"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input:  torch.Size([64, 45])\n",
            "Output:  torch.Size([64, 98])\n",
            "Encoder Output:  torch.Size([33, 64, 1024])\n",
            "Encoder Hidden:  torch.Size([1, 64, 1024])\n",
            "Decoder Input:  torch.Size([64, 1])\n",
            "--------\n",
            "Prediction:  torch.Size([64, 20437])\n",
            "Decoder Hidden:  torch.Size([1, 64, 1024])\n",
            "torch.Size([64, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2-KpvFEa993",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    \"\"\" Only consider non-zero inputs in the loss; mask needed \"\"\"\n",
        "    #mask = 1 - np.equal(real, 0) # assign 0 to all above 0 and 1 to all 0s\n",
        "    #print(mask)\n",
        "    mask = real.ge(1).type(torch.cuda.FloatTensor)\n",
        "    \n",
        "    loss_ = criterion(pred, real) * mask \n",
        "    return torch.mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNbCvOywbAu8",
        "colab_type": "code",
        "outputId": "788cdba7-8e2d-42b3-dddd-704b349616de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "## TODO: Combine the encoder and decoder into one class\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, units, BATCH_SIZE)\n",
        "\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), \n",
        "                       lr=0.001)\n",
        "device"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8pb4PnKbFiy",
        "colab_type": "text"
      },
      "source": [
        "##Train\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtmsZZcqbHuG",
        "colab_type": "code",
        "outputId": "a3cd3ecd-9af8-4dd1-f0bd-8811886e9887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 90\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    \n",
        "    total_loss = 0\n",
        "    \n",
        "    for (batch, (inp, targ, inp_len)) in enumerate(dataset):\n",
        "        loss = 0\n",
        "        \n",
        "        xs, ys, lens = sort_batch(inp, targ, inp_len)\n",
        "        enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "        \n",
        "        for t in range(1, ys.size(1)):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                         dec_hidden.to(device), \n",
        "                                         enc_output.to(device))\n",
        "            loss += loss_function(ys[:, t].to(device), predictions.to(device))\n",
        "            #loss += loss_\n",
        "            dec_input = ys[:, t].unsqueeze(1)\n",
        "            \n",
        "        \n",
        "        batch_loss = (loss / int(ys.size(1)))\n",
        "        total_loss += batch_loss\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "\n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.detach().item()))\n",
        "        \n",
        "    if epoch%5==0:\n",
        "      path1 = F\"/gdrive/My Drive/{'encoder2.torch'}\" \n",
        "      torch.save(encoder, path1)\n",
        "      path2 = F\"/gdrive/My Drive/{'decoder2.torch'}\" \n",
        "      torch.save(decoder, path2)\n",
        "    ### TODO: Save checkpoint for model\n",
        "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / N_BATCH))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 3.6868\n",
            "Epoch 1 Batch 100 Loss 1.3537\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 1.6303\n",
            "Time taken for 1 epoch 114.65635681152344 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.3097\n",
            "Epoch 2 Batch 100 Loss 1.4145\n",
            "Epoch 2 Loss 1.3012\n",
            "Time taken for 1 epoch 114.4038622379303 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.0324\n",
            "Epoch 3 Batch 100 Loss 1.1155\n",
            "Epoch 3 Loss 1.0942\n",
            "Time taken for 1 epoch 114.39831829071045 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.9261\n",
            "Epoch 4 Batch 100 Loss 0.9360\n",
            "Epoch 4 Loss 0.9160\n",
            "Time taken for 1 epoch 114.6073169708252 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.7102\n",
            "Epoch 5 Batch 100 Loss 0.6907\n",
            "Epoch 5 Loss 0.7755\n",
            "Time taken for 1 epoch 114.13746166229248 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6721\n",
            "Epoch 6 Batch 100 Loss 0.6963\n",
            "Epoch 6 Loss 0.6650\n",
            "Time taken for 1 epoch 114.54607081413269 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.5704\n",
            "Epoch 7 Batch 100 Loss 0.5761\n",
            "Epoch 7 Loss 0.5677\n",
            "Time taken for 1 epoch 114.32960677146912 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.4485\n",
            "Epoch 8 Batch 100 Loss 0.5413\n",
            "Epoch 8 Loss 0.4866\n",
            "Time taken for 1 epoch 114.19988322257996 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.3973\n",
            "Epoch 9 Batch 100 Loss 0.4391\n",
            "Epoch 9 Loss 0.4102\n",
            "Time taken for 1 epoch 113.94713735580444 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.3372\n",
            "Epoch 10 Batch 100 Loss 0.2759\n",
            "Epoch 10 Loss 0.3493\n",
            "Time taken for 1 epoch 114.326730966568 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2532\n",
            "Epoch 11 Batch 100 Loss 0.2672\n",
            "Epoch 11 Loss 0.2817\n",
            "Time taken for 1 epoch 114.72981214523315 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2251\n",
            "Epoch 12 Batch 100 Loss 0.2245\n",
            "Epoch 12 Loss 0.2262\n",
            "Time taken for 1 epoch 114.57853317260742 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2078\n",
            "Epoch 13 Batch 100 Loss 0.1889\n",
            "Epoch 13 Loss 0.1840\n",
            "Time taken for 1 epoch 114.43270111083984 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.1420\n",
            "Epoch 14 Batch 100 Loss 0.1443\n",
            "Epoch 14 Loss 0.1492\n",
            "Time taken for 1 epoch 114.68221616744995 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.0849\n",
            "Epoch 15 Batch 100 Loss 0.1361\n",
            "Epoch 15 Loss 0.1238\n",
            "Time taken for 1 epoch 114.38734221458435 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.1195\n",
            "Epoch 16 Batch 100 Loss 0.1021\n",
            "Epoch 16 Loss 0.1128\n",
            "Time taken for 1 epoch 115.12974524497986 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.0906\n",
            "Epoch 17 Batch 100 Loss 0.0901\n",
            "Epoch 17 Loss 0.0989\n",
            "Time taken for 1 epoch 114.8178436756134 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.0758\n",
            "Epoch 18 Batch 100 Loss 0.0677\n",
            "Epoch 18 Loss 0.0858\n",
            "Time taken for 1 epoch 114.55298376083374 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.1052\n",
            "Epoch 19 Batch 100 Loss 0.0632\n",
            "Epoch 19 Loss 0.0816\n",
            "Time taken for 1 epoch 115.0127341747284 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.0645\n",
            "Epoch 20 Batch 100 Loss 0.0920\n",
            "Epoch 20 Loss 0.0751\n",
            "Time taken for 1 epoch 114.58979034423828 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.0706\n",
            "Epoch 21 Batch 100 Loss 0.0736\n",
            "Epoch 21 Loss 0.0750\n",
            "Time taken for 1 epoch 115.18910098075867 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.0676\n",
            "Epoch 22 Batch 100 Loss 0.0590\n",
            "Epoch 22 Loss 0.0647\n",
            "Time taken for 1 epoch 114.68316650390625 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.0582\n",
            "Epoch 23 Batch 100 Loss 0.0720\n",
            "Epoch 23 Loss 0.0588\n",
            "Time taken for 1 epoch 114.52488899230957 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.0601\n",
            "Epoch 24 Batch 100 Loss 0.0571\n",
            "Epoch 24 Loss 0.0507\n",
            "Time taken for 1 epoch 114.63408875465393 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.0373\n",
            "Epoch 25 Batch 100 Loss 0.0493\n",
            "Epoch 25 Loss 0.0470\n",
            "Time taken for 1 epoch 114.59731411933899 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.0567\n",
            "Epoch 26 Batch 100 Loss 0.0572\n",
            "Epoch 26 Loss 0.0538\n",
            "Time taken for 1 epoch 115.18104100227356 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.0328\n",
            "Epoch 27 Batch 100 Loss 0.0776\n",
            "Epoch 27 Loss 0.0631\n",
            "Time taken for 1 epoch 114.76765871047974 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.0387\n",
            "Epoch 28 Batch 100 Loss 0.0787\n",
            "Epoch 28 Loss 0.0584\n",
            "Time taken for 1 epoch 114.70454692840576 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.0582\n",
            "Epoch 29 Batch 100 Loss 0.0471\n",
            "Epoch 29 Loss 0.0537\n",
            "Time taken for 1 epoch 114.69275665283203 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.0634\n",
            "Epoch 30 Batch 100 Loss 0.0551\n",
            "Epoch 30 Loss 0.0533\n",
            "Time taken for 1 epoch 114.69225645065308 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.0552\n",
            "Epoch 31 Batch 100 Loss 0.0587\n",
            "Epoch 31 Loss 0.0450\n",
            "Time taken for 1 epoch 115.2146635055542 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.0319\n",
            "Epoch 32 Batch 100 Loss 0.0379\n",
            "Epoch 32 Loss 0.0385\n",
            "Time taken for 1 epoch 114.75423812866211 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.0243\n",
            "Epoch 33 Batch 100 Loss 0.0335\n",
            "Epoch 33 Loss 0.0361\n",
            "Time taken for 1 epoch 114.81131505966187 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.0300\n",
            "Epoch 34 Batch 100 Loss 0.0267\n",
            "Epoch 34 Loss 0.0329\n",
            "Time taken for 1 epoch 114.79225993156433 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.0307\n",
            "Epoch 35 Batch 100 Loss 0.0266\n",
            "Epoch 35 Loss 0.0307\n",
            "Time taken for 1 epoch 114.94129872322083 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.0325\n",
            "Epoch 36 Batch 100 Loss 0.0245\n",
            "Epoch 36 Loss 0.0385\n",
            "Time taken for 1 epoch 115.39926409721375 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.0477\n",
            "Epoch 37 Batch 100 Loss 0.0794\n",
            "Epoch 37 Loss 0.0668\n",
            "Time taken for 1 epoch 114.80548334121704 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.0520\n",
            "Epoch 38 Batch 100 Loss 0.0549\n",
            "Epoch 38 Loss 0.0606\n",
            "Time taken for 1 epoch 114.87783312797546 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.0268\n",
            "Epoch 39 Batch 100 Loss 0.0505\n",
            "Epoch 39 Loss 0.0457\n",
            "Time taken for 1 epoch 114.91115736961365 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.0273\n",
            "Epoch 40 Batch 100 Loss 0.0336\n",
            "Epoch 40 Loss 0.0368\n",
            "Time taken for 1 epoch 114.80324959754944 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.0385\n",
            "Epoch 41 Batch 100 Loss 0.0381\n",
            "Epoch 41 Loss 0.0310\n",
            "Time taken for 1 epoch 115.45072507858276 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.0282\n",
            "Epoch 42 Batch 100 Loss 0.0429\n",
            "Epoch 42 Loss 0.0357\n",
            "Time taken for 1 epoch 115.09318995475769 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.0498\n",
            "Epoch 43 Batch 100 Loss 0.0359\n",
            "Epoch 43 Loss 0.0402\n",
            "Time taken for 1 epoch 114.57732081413269 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.0299\n",
            "Epoch 44 Batch 100 Loss 0.0238\n",
            "Epoch 44 Loss 0.0347\n",
            "Time taken for 1 epoch 114.87192273139954 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.0245\n",
            "Epoch 45 Batch 100 Loss 0.0214\n",
            "Epoch 45 Loss 0.0299\n",
            "Time taken for 1 epoch 114.78752994537354 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.0374\n",
            "Epoch 46 Batch 100 Loss 0.0488\n",
            "Epoch 46 Loss 0.0373\n",
            "Time taken for 1 epoch 115.47849678993225 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.0261\n",
            "Epoch 47 Batch 100 Loss 0.0479\n",
            "Epoch 47 Loss 0.0406\n",
            "Time taken for 1 epoch 115.18862318992615 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.0403\n",
            "Epoch 48 Batch 100 Loss 0.0340\n",
            "Epoch 48 Loss 0.0330\n",
            "Time taken for 1 epoch 114.82026648521423 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.0183\n",
            "Epoch 49 Batch 100 Loss 0.0240\n",
            "Epoch 49 Loss 0.0292\n",
            "Time taken for 1 epoch 114.88494110107422 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.0238\n",
            "Epoch 50 Batch 100 Loss 0.0296\n",
            "Epoch 50 Loss 0.0259\n",
            "Time taken for 1 epoch 115.03122472763062 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.0340\n",
            "Epoch 51 Batch 100 Loss 0.0168\n",
            "Epoch 51 Loss 0.0239\n",
            "Time taken for 1 epoch 115.33973574638367 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.0328\n",
            "Epoch 52 Batch 100 Loss 0.0155\n",
            "Epoch 52 Loss 0.0271\n",
            "Time taken for 1 epoch 114.64523720741272 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.0604\n",
            "Epoch 53 Batch 100 Loss 0.0667\n",
            "Epoch 53 Loss 0.0608\n",
            "Time taken for 1 epoch 114.30976343154907 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.0433\n",
            "Epoch 54 Batch 100 Loss 0.0391\n",
            "Epoch 54 Loss 0.0471\n",
            "Time taken for 1 epoch 114.53051853179932 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.0300\n",
            "Epoch 55 Batch 100 Loss 0.0331\n",
            "Epoch 55 Loss 0.0361\n",
            "Time taken for 1 epoch 114.41861128807068 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.0295\n",
            "Epoch 56 Batch 100 Loss 0.0269\n",
            "Epoch 56 Loss 0.0288\n",
            "Time taken for 1 epoch 114.99829745292664 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.0206\n",
            "Epoch 57 Batch 100 Loss 0.0231\n",
            "Epoch 57 Loss 0.0230\n",
            "Time taken for 1 epoch 114.51077699661255 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.0185\n",
            "Epoch 58 Batch 100 Loss 0.0216\n",
            "Epoch 58 Loss 0.0194\n",
            "Time taken for 1 epoch 114.56866908073425 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.0164\n",
            "Epoch 59 Batch 100 Loss 0.0191\n",
            "Epoch 59 Loss 0.0181\n",
            "Time taken for 1 epoch 114.19596576690674 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.0129\n",
            "Epoch 60 Batch 100 Loss 0.0105\n",
            "Epoch 60 Loss 0.0199\n",
            "Time taken for 1 epoch 114.11888074874878 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.0257\n",
            "Epoch 61 Batch 100 Loss 0.0209\n",
            "Epoch 61 Loss 0.0231\n",
            "Time taken for 1 epoch 114.69177198410034 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.0194\n",
            "Epoch 62 Batch 100 Loss 0.0325\n",
            "Epoch 62 Loss 0.0245\n",
            "Time taken for 1 epoch 114.30047106742859 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.0263\n",
            "Epoch 63 Batch 100 Loss 0.0283\n",
            "Epoch 63 Loss 0.0330\n",
            "Time taken for 1 epoch 114.5350706577301 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.0613\n",
            "Epoch 64 Batch 100 Loss 0.0297\n",
            "Epoch 64 Loss 0.0492\n",
            "Time taken for 1 epoch 114.21625900268555 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.0922\n",
            "Epoch 65 Batch 100 Loss 0.0917\n",
            "Epoch 65 Loss 0.0765\n",
            "Time taken for 1 epoch 114.32439398765564 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.0373\n",
            "Epoch 66 Batch 100 Loss 0.0426\n",
            "Epoch 66 Loss 0.0453\n",
            "Time taken for 1 epoch 115.20720958709717 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.0368\n",
            "Epoch 67 Batch 100 Loss 0.0307\n",
            "Epoch 67 Loss 0.0295\n",
            "Time taken for 1 epoch 114.50088167190552 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.0306\n",
            "Epoch 68 Batch 100 Loss 0.0263\n",
            "Epoch 68 Loss 0.0221\n",
            "Time taken for 1 epoch 114.63719272613525 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.0114\n",
            "Epoch 69 Batch 100 Loss 0.0199\n",
            "Epoch 69 Loss 0.0173\n",
            "Time taken for 1 epoch 114.2324628829956 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.0109\n",
            "Epoch 70 Batch 100 Loss 0.0163\n",
            "Epoch 70 Loss 0.0152\n",
            "Time taken for 1 epoch 114.15966963768005 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0149\n",
            "Epoch 71 Batch 100 Loss 0.0072\n",
            "Epoch 71 Loss 0.0134\n",
            "Time taken for 1 epoch 114.84244084358215 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.0091\n",
            "Epoch 72 Batch 100 Loss 0.0131\n",
            "Epoch 72 Loss 0.0128\n",
            "Time taken for 1 epoch 114.31098580360413 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0049\n",
            "Epoch 73 Batch 100 Loss 0.0195\n",
            "Epoch 73 Loss 0.0131\n",
            "Time taken for 1 epoch 114.43326187133789 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.0103\n",
            "Epoch 74 Batch 100 Loss 0.0188\n",
            "Epoch 74 Loss 0.0134\n",
            "Time taken for 1 epoch 114.17920231819153 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0140\n",
            "Epoch 75 Batch 100 Loss 0.0117\n",
            "Epoch 75 Loss 0.0152\n",
            "Time taken for 1 epoch 114.490713596344 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0149\n",
            "Epoch 76 Batch 100 Loss 0.0263\n",
            "Epoch 76 Loss 0.0299\n",
            "Time taken for 1 epoch 114.71376442909241 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0615\n",
            "Epoch 77 Batch 100 Loss 0.0663\n",
            "Epoch 77 Loss 0.0620\n",
            "Time taken for 1 epoch 114.3684937953949 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.0446\n",
            "Epoch 78 Batch 100 Loss 0.0460\n",
            "Epoch 78 Loss 0.0532\n",
            "Time taken for 1 epoch 114.34967279434204 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0320\n",
            "Epoch 79 Batch 100 Loss 0.0347\n",
            "Epoch 79 Loss 0.0381\n",
            "Time taken for 1 epoch 114.34406352043152 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0213\n",
            "Epoch 80 Batch 100 Loss 0.0383\n",
            "Epoch 80 Loss 0.0327\n",
            "Time taken for 1 epoch 114.27235245704651 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0278\n",
            "Epoch 81 Batch 100 Loss 0.0267\n",
            "Epoch 81 Loss 0.0715\n",
            "Time taken for 1 epoch 115.04264831542969 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.0686\n",
            "Epoch 82 Batch 100 Loss 0.0605\n",
            "Epoch 82 Loss 0.0723\n",
            "Time taken for 1 epoch 114.3477885723114 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0327\n",
            "Epoch 83 Batch 100 Loss 0.0404\n",
            "Epoch 83 Loss 0.0324\n",
            "Time taken for 1 epoch 114.37711238861084 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0190\n",
            "Epoch 84 Batch 100 Loss 0.0345\n",
            "Epoch 84 Loss 0.0195\n",
            "Time taken for 1 epoch 114.31609034538269 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0185\n",
            "Epoch 85 Batch 100 Loss 0.0135\n",
            "Epoch 85 Loss 0.0138\n",
            "Time taken for 1 epoch 114.28922486305237 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0102\n",
            "Epoch 86 Batch 100 Loss 0.0133\n",
            "Epoch 86 Loss 0.0110\n",
            "Time taken for 1 epoch 114.76436400413513 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0133\n",
            "Epoch 87 Batch 100 Loss 0.0101\n",
            "Epoch 87 Loss 0.0094\n",
            "Time taken for 1 epoch 114.52304172515869 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0061\n",
            "Epoch 88 Batch 100 Loss 0.0072\n",
            "Epoch 88 Loss 0.0084\n",
            "Time taken for 1 epoch 114.06874465942383 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0062\n",
            "Epoch 89 Batch 100 Loss 0.0102\n",
            "Epoch 89 Loss 0.0076\n",
            "Time taken for 1 epoch 114.51840543746948 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0086\n",
            "Epoch 90 Batch 100 Loss 0.0142\n",
            "Epoch 90 Loss 0.0074\n",
            "Time taken for 1 epoch 114.44893074035645 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2zpJDTqnaKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "04e8e4ba-738a-4318-9e4c-cebc3f5689e7"
      },
      "source": [
        "path1 = F\"/gdrive/My Drive/{'encoder2.torch'}\" \n",
        "torch.save(encoder, path1)\n",
        "path2 = F\"/gdrive/My Drive/{'decoder2.torch'}\" \n",
        "torch.save(decoder, path2)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type GRU. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbBrQaJlvZNK",
        "colab_type": "text"
      },
      "source": [
        "##Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2VklCLNbMaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path1 = F\"/gdrive/My Drive/{'encoder.torch'}\" \n",
        "# torch.save(encoder, path1)\n",
        "# path2 = F\"/gdrive/My Drive/{'decoder.torch'}\" \n",
        "# torch.save(decoder, path2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Xlt6A094Wa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "08638c15-d562-451a-c59f-bde02ca438e0"
      },
      "source": [
        "import random\n",
        "for i in range(20):\n",
        "      select=random.sample(range(10307),3)\n",
        "      xs=[3]+select+[2]\n",
        "      print([inp_lang.idx2word[i] for i in xs])\n",
        "\n",
        "      xs = [ xs for i in range(BATCH_SIZE)]\n",
        "      xs=torch.LongTensor(xs)\n",
        "      xs=xs.t()\n",
        "      lens=len(xs)\n",
        "      lens = [ lens for i in range(BATCH_SIZE)]\n",
        "      lens=torch.LongTensor(lens)\n",
        "      #print(xs.size(),lens.size())\n",
        "\n",
        "      enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "      res=[]\n",
        "      for t in range(1, ys.size(1)):\n",
        "        predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                          dec_hidden.to(device), \n",
        "                                          enc_output.to(device))\n",
        "\n",
        "        dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
        "        p=predictions.argmax(dim=1).tolist()\n",
        "        res.append(targ_lang.idx2word[p[0]])\n",
        "\n",
        "      sentence=''\n",
        "      for i in res:\n",
        "        if i!='<end>' and i!='<pad>':\n",
        "          sentence+= i+' '\n",
        "      print(sentence)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start>', 'hulk', 'copenhagen', 'promotion', '<end>']\n",
            "what did the like a fancy french of a what promotion . \n",
            "['<start>', 'chancellor', 'cockatoo', 'bocelli', '<end>']\n",
            "so , a german has unveiled a free has officially confirmed the first has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the medicine has officially confirmed the \n",
            "['<start>', 'doorway', 'baskets', 'mannequin', '<end>']\n",
            "why do we all know who knew that i like to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call it was trying to call \n",
            "['<start>', 'bowl', 'pitcher', 'jimi', '<end>']\n",
            "what did the super bowl , super bowl ? i m not what i m a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these is getting a pile of these \n",
            "['<start>', 'competitiveness', 'predicate', 'osbourne', '<end>']\n",
            "why do you get when you can t so fat , so fat , so fat , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , so , \n",
            "['<start>', 'km', 'grizzly', 'buffets', '<end>']\n",
            "what do you call a pretty much tired of them . \n",
            "['<start>', 'duracell', 'bolts', 'miners', '<end>']\n",
            "you have you think that you can t wait to you go out of a french are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go to the only are you go \n",
            "['<start>', 'communism', 'churches', 'invent', '<end>']\n",
            "why are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there are you get why is there \n",
            "['<start>', 'emotions', 'iconic', 'pictures', '<end>']\n",
            "i like to his own . if they re going to them . \n",
            "['<start>', 'increments', 'mold', 'palace', '<end>']\n",
            "if there is a huge egg s has found a happy new speaker is your left . \n",
            "['<start>', 'crows', 'dort', 'quitting', '<end>']\n",
            "why do you get a us who is back to be a simple . \n",
            "['<start>', 'confusion', 'ballpark', 'popemobile', '<end>']\n",
            "we need a new middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the middle of the \n",
            "['<start>', 'months', 'dickiedoo', 'conditioner', '<end>']\n",
            "they ve been told me . \n",
            "['<start>', 'knees', 'peer', 'gluten', '<end>']\n",
            "if a new black guy who are not selling a million hits the living you can be here . \n",
            "['<start>', 'female', 'murders', 'handel', '<end>']\n",
            "the new black was the future will be a two fish \n",
            "['<start>', 'bies', 'cottage', 'ignorance', '<end>']\n",
            "how do you get a little old , how many of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the other are out of the \n",
            "['<start>', 'cattrall', 'attract', 'oscar', '<end>']\n",
            "a white , the top of the top of a white giraffe made a oscar has nominated and nominated and wearing a top of the top of a white giraffe made a oscar has nominated and nominated and wearing a top of the top of a white giraffe made a oscar has nominated and nominated and wearing a top of the top of a white giraffe made a oscar has nominated and nominated and wearing a top of the top of a white giraffe made a oscar has nominated and nominated and wearing a top of the \n",
            "['<start>', 'presidency', 'guynecology', 'locker', '<end>']\n",
            "how do you get when a recent buy has been talking about his own talking ? \n",
            "['<start>', 'children', 'boyfriend', 'disagreements', '<end>']\n",
            "what goes my children was . \n",
            "['<start>', 'consult', 'mater', 'etc', '<end>']\n",
            "it s been reported that the oldest have is sure you have to tell him bernie is still be before that . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NEKiZSyulqh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5935c4b4-ebad-4559-bb49-e0015f52f544"
      },
      "source": [
        "#xs=[inp_lang.word2idx[i] for i in topic_words.split()]\n",
        "xs = [ xs for i in range(BATCH_SIZE)]\n",
        "xs=torch.LongTensor(xs)\n",
        "xs=xs.t()\n",
        "lens=len(xs)\n",
        "lens = [ lens for i in range(BATCH_SIZE)]\n",
        "lens=torch.LongTensor(lens)\n",
        "\n",
        "xs.size()\n"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0VoRmi7CED1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc_output, enc_hidden = encoder(xs.to(device), lens, device)\n",
        "dec_hidden = enc_hidden\n",
        "dec_input = torch.tensor([[targ_lang.word2idx['<start>']]] * BATCH_SIZE)\n",
        "res=[]\n",
        "for t in range(1, ys.size(1)):\n",
        "  predictions, dec_hidden, _ = decoder(dec_input.to(device), \n",
        "                                    dec_hidden.to(device), \n",
        "                                    enc_output.to(device))\n",
        "\n",
        "  dec_input = predictions.argmax(dim=1).unsqueeze(1)\n",
        "  p=predictions.argmax(dim=1).tolist()\n",
        "  res.append(targ_lang.idx2word[p[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNS7uNGpvFhd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c0676700-a49b-414a-a9f5-74435968261c"
      },
      "source": [
        "sentence=''\n",
        "for i in res:\n",
        "  if i!='<end>' and i!='<pad>':\n",
        "    sentence+= i+' '\n",
        "sentence"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'i can tell me some of . '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlUoIqOBwFCn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58e4bc42-fb0a-4132-d99d-15d5c9221e90"
      },
      "source": [
        "jo=[]\n",
        "for i in ys[0].tolist():\n",
        "  jo.append(targ_lang.idx2word[i])\n",
        "jo"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start>',\n",
              " 'marcus',\n",
              " 'bachmann',\n",
              " 'wrote',\n",
              " 'an',\n",
              " 'open',\n",
              " 'letter',\n",
              " 'to',\n",
              " 'conservatives',\n",
              " 'describing',\n",
              " 'his',\n",
              " 'wife',\n",
              " 'michele',\n",
              " 'as',\n",
              " 'rock',\n",
              " 'solid',\n",
              " '.',\n",
              " 'probably',\n",
              " 'not',\n",
              " 'helping',\n",
              " 'was',\n",
              " 'that',\n",
              " 'he',\n",
              " 'then',\n",
              " 'wrote',\n",
              " ',',\n",
              " 'as',\n",
              " 'rock',\n",
              " 'solid',\n",
              " 'as',\n",
              " 'taylor',\n",
              " 'lautner',\n",
              " 's',\n",
              " 'yummy',\n",
              " 'abs',\n",
              " '.',\n",
              " '<end>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    }
  ]
}